{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a60f3e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def train_bert_model():\n",
    "    \"\"\"\n",
    "    This function downloads the dataset, preprocesses the data,\n",
    "    fine-tunes a BERT model for fake news classification,\n",
    "    and saves the trained model and tokenizer.\n",
    "    \"\"\"\n",
    "    # --- 1. Load Dataset ---\n",
    "    # Using a well-known fake news dataset from Kaggle.\n",
    "    # Note: This file is ~44MB.\n",
    "    logger.info(\"Downloading dataset...\")\n",
    "    try:\n",
    "        # The dataset contains two files: True.csv and Fake.csv\n",
    "        true_df = pd.read_csv(\"../News_dataset/True.csv\")\n",
    "        fake_df = pd.read_csv(\"../News_dataset/Fake.csv\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to download or read the dataset. Error: {e}\")\n",
    "        logger.error(\"Please ensure you have an internet connection and the URL is correct.\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Preprocess Data ---\n",
    "    logger.info(\"Preprocessing data...\")\n",
    "    # Add labels: 1 for 'real', 0 for 'fake'\n",
    "    true_df['label'] = 1\n",
    "    fake_df['label'] = 0\n",
    "\n",
    "    # Combine the dataframes\n",
    "    df = pd.concat([true_df, fake_df], ignore_index=True)\n",
    "\n",
    "    # Combine title and text for a more comprehensive input\n",
    "    df['text'] = df['title'] + \" \" + df['text']\n",
    "    df = df[['text', 'label']]\n",
    "\n",
    "    # Shuffle the dataset\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # For demonstration, we'll use a smaller subset of the data to speed up training.\n",
    "    # You can increase this for better accuracy.\n",
    "    df = df.head(5000)\n",
    "\n",
    "    # Split into training and validation sets\n",
    "    X = df['text'].tolist()\n",
    "    y = df['label'].tolist()\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # --- 3. Tokenization ---\n",
    "    logger.info(\"Tokenizing data...\")\n",
    "    # Use the 'bert-base-uncased' tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Tokenize the text data\n",
    "    train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=128)\n",
    "    val_encodings = tokenizer(X_val, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "    # Convert to TensorFlow datasets\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        dict(train_encodings),\n",
    "        y_train\n",
    "    ))\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        dict(val_encodings),\n",
    "        y_val\n",
    "    ))\n",
    "\n",
    "    # --- 4. Model Training ---\n",
    "    logger.info(\"Initializing and training the BERT model...\")\n",
    "    # Load the pre-trained BERT model for sequence classification\n",
    "    model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "    # Define training parameters\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "    # Batch and shuffle the datasets\n",
    "    train_dataset_batched = train_dataset.shuffle(1000).batch(16)\n",
    "    val_dataset_batched = val_dataset.batch(16)\n",
    "\n",
    "    # Fine-tune the model\n",
    "    # For a real-world scenario, you might train for more epochs (e.g., 3-5).\n",
    "    # We use 1 epoch here for a quicker demonstration.\n",
    "    model.fit(train_dataset_batched, epochs=1, validation_data=val_dataset_batched)\n",
    "\n",
    "    # --- 5. Save the Model and Tokenizer ---\n",
    "    logger.info(\"Saving the fine-tuned model and tokenizer...\")\n",
    "    save_directory = './saved_model'\n",
    "    model.save_pretrained(save_directory)\n",
    "    tokenizer.save_pretrained(save_directory)\n",
    "    logger.info(f\"Model and tokenizer saved in '{save_directory}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d77382e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/suyashmishra/miniconda3/envs/project/lib/python3.11/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/suyashmishra/miniconda3/envs/project/lib/python3.11/site-packages (from ipywidgets) (9.3.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/suyashmishra/miniconda3/envs/project/lib/python3.11/site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: decorator in /home/suyashmishra/miniconda3/envs/project/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /home/suyashmishra/miniconda3/envs/project/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/suyashmishra/miniconda3/envs/project/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/suyashmishra/miniconda3/envs/project/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/suyashmishra/miniconda3/envs/project/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/suyashmishra/miniconda3/envs/project/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/suyashmishra/miniconda3/envs/project/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /home/suyashmishra/miniconda3/envs/project/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in /home/suyashmishra/miniconda3/envs/project/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.14.0)\n",
      "Requirement already satisfied: wcwidth in /home/suyashmishra/miniconda3/envs/project/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/suyashmishra/miniconda3/envs/project/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/suyashmishra/miniconda3/envs/project/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/suyashmishra/miniconda3/envs/project/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/suyashmishra/miniconda3/envs/project/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /home/suyashmishra/miniconda3/envs/project/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Downloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "Downloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m981.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [ipywidgets]3\u001b[0m [ipywidgets]\n",
      "\u001b[1A\u001b[2KSuccessfully installed ipywidgets-8.1.7 jupyterlab_widgets-3.0.15 widgetsnbextension-4.0.14\n"
     ]
    }
   ],
   "source": [
    "! pip install ipywidgets --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92592494",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloading dataset...\n",
      "INFO:__main__:Preprocessing data...\n",
      "INFO:__main__:Tokenizing data...\n",
      "W0000 00:00:1757173118.718134  100959 gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "INFO:__main__:Initializing and training the BERT model...\n",
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 1141s 4s/step - loss: 0.0515 - accuracy: 0.9835 - val_loss: 0.1363 - val_accuracy: 0.9690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Saving the fine-tuned model and tokenizer...\n",
      "INFO:__main__:Model and tokenizer saved in './saved_model'\n"
     ]
    }
   ],
   "source": [
    "train_bert_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
